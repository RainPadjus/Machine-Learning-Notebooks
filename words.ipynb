{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rando\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "from tinygrad.tensor import Tensor, Device\n",
    "from tinygrad.nn import optim, BatchNorm2d\n",
    "from extra.training import train, evaluate\n",
    "from datasets import fetch_mnist\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "import time\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.optim import Adam\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location  \\\n",
       "id                    \n",
       "1      NaN      NaN   \n",
       "4      NaN      NaN   \n",
       "5      NaN      NaN   \n",
       "6      NaN      NaN   \n",
       "7      NaN      NaN   \n",
       "\n",
       "                                                                                                                                     text  \\\n",
       "id                                                                                                                                          \n",
       "1                                                                   Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "4                                                                                                  Forest fire near La Ronge Sask. Canada   \n",
       "5   All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "6                                                                       13,000 people receive #wildfires evacuation orders in California    \n",
       "7                                                Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "    target  \n",
       "id          \n",
       "1        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  "
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data = pd.read_csv('data_tw/train.csv').set_index(\"id\")\n",
    "train_data.head()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def embed(text):\n",
    "    emb = np.zeros(18613)\n",
    "    for i in text:\n",
    "        try:\n",
    "            emb[voc.index(i)]=1\n",
    "        except:\n",
    "            pass \n",
    "    return emb\n",
    "\n",
    "\n",
    "test = pd.read_csv('data_tw/test.csv').set_index(\"id\")\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
    "test['text'] = test['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "all_text = ' '.join(train_data['text'])\n",
    "all_text += ' '.join(test['text'])\n",
    "\n",
    "voc = list(set(all_text.split()))\n",
    "\n",
    "train_data['emb'] = train_data['text'].apply(embed)\n",
    "\n",
    "X_train = np.stack(train_data['emb'].apply(lambda x: [int(i) for i in x]).values).astype(np.float32)\n",
    "Y_train = train_data['target'].values.astype(np.uint8)\n",
    "\n",
    "class TwitterNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Tensor.scaled_uniform(18613, 512)  # First hidden layer has 512 nodes\n",
    "        self.l2 = Tensor.scaled_uniform(512, 256)\n",
    "        self.l3 = Tensor.scaled_uniform(256,100)    # Second hidden layer has 256 nodes\n",
    "        self.l4 = Tensor.scaled_uniform(100, 2)      # Output layer has 2 nodes\n",
    "\n",
    "    def parameters(self):\n",
    "        return optim.get_parameters(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.dot(self.l1).leakyrelu()        # Input to first hidden layer\n",
    "        x = x.dot(self.l2).leakyrelu()\n",
    "        x = x.dot(self.l3).leakyrelu()        # First hidden layer to second hidden layer\n",
    "        return x.dot(self.l4).log_softmax()  # Second hidden layer to output\n",
    "\n",
    "model = TwitterNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "train(model, X_train, Y_train, optimizer, BS=100, steps=1000)                                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Feature Engineering\n",
    "def get_word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "train_data = pd.read_csv('data_tw/train.csv').set_index(\"id\")\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
    "train_data['word_count'] = train_data['text'].apply(get_word_count)\n",
    "train_data['has_keyword'] = ~train_data['keyword'].isnull()\n",
    "train_data['has_location'] = ~train_data['location'].isnull()\n",
    "\n",
    "test = pd.read_csv('data_tw/test.csv').set_index(\"id\")\n",
    "test['text'] = test['text'].apply(preprocess_text)\n",
    "test['word_count'] = test['text'].apply(get_word_count)\n",
    "test['has_keyword'] = ~test['keyword'].isnull()\n",
    "test['has_location'] = ~test['location'].isnull()\n",
    "\n",
    "all_text = ' '.join(train_data['text'])\n",
    "all_text += ' '.join(test['text'])\n",
    "\n",
    "voc = list(set(all_text.split()))\n",
    "\n",
    "def embed(text):\n",
    "    emb = np.zeros(18613)\n",
    "    for i in text:\n",
    "        try:\n",
    "            emb[voc.index(i)]=1\n",
    "        except:\n",
    "            pass \n",
    "    return emb\n",
    "\n",
    "train_data['emb'] = train_data['text'].apply(embed)\n",
    "\n",
    "# Update feature vector to include additional features\n",
    "def get_feature_vector(row):\n",
    "    emb = list(embed(row['text']))\n",
    "    emb.append(row['word_count'])\n",
    "    emb.append(int(row['has_keyword']))\n",
    "    emb.append(int(row['has_location']))\n",
    "    return emb\n",
    "\n",
    "X_train = np.stack(train_data.apply(get_feature_vector, axis=1).values).astype(np.float32)\n",
    "Y_train = train_data['target'].values.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss -13.74 accuracy 0.63: 100%|██████████| 1000/1000 [01:29<00:00, 11.19it/s]\n"
     ]
    }
   ],
   "source": [
    "class TwitterNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Tensor.scaled_uniform(18613 + 3, 512)  # Adjust input size to match feature vector\n",
    "        self.l2 = Tensor.scaled_uniform(512, 256)\n",
    "        self.l3 = Tensor.scaled_uniform(256,100) \n",
    "        self.l4 = Tensor.scaled_uniform(100, 2)  \n",
    "\n",
    "    def parameters(self):\n",
    "        return optim.get_parameters(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.dot(self.l1).relu()       \n",
    "        x = x.dot(self.l2).relu()\n",
    "        x = x.dot(self.l3).relu()        \n",
    "        return x.dot(self.l4)\n",
    "\n",
    "model = TwitterNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "train(model, X_train, Y_train, optimizer, BS=100, steps=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data_tw/train.csv')\n",
    "test_df = pd.read_csv('data_tw/test.csv')\n",
    "train_df['text'] = 'TEXT: ' + train_df['text'].fillna(' ') + ' LOCATION: ' + train_df['location'].fillna(' ') + ' KEYWORD: ' + train_df['keyword'].fillna(' ')\n",
    "test_df['text'] = 'TEXT: ' + test_df['text'].fillna(' ') + ' LOCATION: ' + test_df['location'].fillna(' ') + ' KEYWORD: ' + test_df['keyword'].fillna(' ')\n",
    "train_df.drop(['location', 'keyword', 'id'], axis=1, inplace=True)\n",
    "train_df.rename(columns={'target': 'label'}, inplace=True)\n",
    "train_df['is_valid'] = False\n",
    "\n",
    "test_df.drop(['location', 'keyword', 'id'], axis=1, inplace=True)\n",
    "test_df.rename(columns={'target': 'label'}, inplace=True)\n",
    "\n",
    "train_df.head()\n",
    "num_rows_to_change = int(len(train_df) * 0.2)\n",
    "random_indices = np.random.choice(train_df.index, size=num_rows_to_change, replace=False)\n",
    "train_df.loc[random_indices, 'is_valid'] = True\n",
    "\n",
    "from fastai.text.all import *\n",
    "\n",
    "twitter_cl = DataBlock(blocks=(TextBlock.from_df('text', seq_len=72), CategoryBlock),\n",
    "                      get_x=ColReader('text'), get_y=ColReader('label'), splitter=ColSplitter())\n",
    "\n",
    "dls = twitter_cl.dataloaders(train_df, bs=64)\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)\n",
    "test_df['label'] = 0\n",
    "t = test_df\n",
    "\n",
    "# Assume df is your DataFrame and 'text' is the column containing text\n",
    "test_dl = learn.dls.test_dl(t['text'])  # Create a test DataLoader\n",
    "preds, _ = learn.get_preds(dl=test_dl)  # Make predictions\n",
    "\n",
    "predicted_labels = preds.argmax(dim=1)\n",
    "t['label'] = predicted_labels\n",
    "\n",
    "t['label'] = t['text'].apply(learn.predict)\n",
    "\n",
    "learn.predict('TEXT: EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn LOCATION:   KEYWORD')\n",
    "sub = pd.read_csv('data_tw/sample_submission.csv').set_index('id')\n",
    "sub['target'] = t.label\n",
    "out = t.drop(['text'], axis=1, inplace=False)\n",
    "out = out.rename(columns={'label': 'target'})\n",
    "out = out.reset_index()\n",
    "\n",
    "# Rename the new column (which is named 'index' by default) to 'id'\n",
    "out = out.rename(columns={'index': 'id'})\n",
    "out\n",
    "out.to_csv('data_tw/out_y.csv', index=False)\n",
    "pd.read_csv('data_tw/out_y.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `n_workers` has to be changed to 0 to avoid getting stuck\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.579023</td>\n",
       "      <td>0.497932</td>\n",
       "      <td>0.767411</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.522218</td>\n",
       "      <td>0.472355</td>\n",
       "      <td>0.773982</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.463258</td>\n",
       "      <td>0.447319</td>\n",
       "      <td>0.793693</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   1       1\n",
       "2   2       1\n",
       "3   3       1\n",
       "4   4       1"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.text.all import *\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data_tw/train.csv')\n",
    "test_df = pd.read_csv('data_tw/test.csv')\n",
    "\n",
    "# Preprocess text\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = 'TEXT: ' + df['text'].fillna(' ') + \\\n",
    "                 ' LOCATION: ' + df['location'].fillna(' ') + \\\n",
    "                 ' KEYWORD: ' + df['keyword'].fillna(' ')\n",
    "    df.drop(['location', 'keyword'], axis=1, inplace=True)\n",
    "    df.rename(columns={'target': 'label'}, inplace=True)\n",
    "\n",
    "\n",
    "# Set validation split\n",
    "num_rows_to_change = int(len(train_df) * 0.2)\n",
    "random_indices = np.random.choice(train_df.index, size=num_rows_to_change, replace=False)\n",
    "train_df['is_valid'] = False\n",
    "train_df.loc[random_indices, 'is_valid'] = True\n",
    "\n",
    "# Define DataBlock and DataLoader\n",
    "twitter_cl = DataBlock(blocks=(TextBlock.from_df('text', seq_len=72), CategoryBlock),\n",
    "                      get_x=ColReader('text'), get_y=ColReader('label'), splitter=ColSplitter())\n",
    "dls = twitter_cl.dataloaders(train_df, bs=64)\n",
    "\n",
    "# Train model\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(2, 1e-2)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_dl = learn.dls.test_dl(test_df['text'])  # Create a test DataLoader\n",
    "preds, _ = learn.get_preds(dl=test_dl)  # Make predictions\n",
    "predicted_labels = preds.argmax(dim=1)\n",
    "test_df['label'] = predicted_labels\n",
    "\n",
    "# Prepare submission\n",
    "sub = pd.read_csv('data_tw/sample_submission.csv').set_index('id')\n",
    "sub['target'] = test_df.label\n",
    "out = test_df.drop(['text'], axis=1, inplace=False)\n",
    "out = out.rename(columns={'label': 'target'})\n",
    "\n",
    "# Save to csv\n",
    "out.to_csv('data_tw/out_y.csv', index=False)\n",
    "pd.read_csv('data_tw/out_y.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
